---
layout: page
title: Research
---
{::options parse_block_html="true" /}
<a name="top"></a> 
<span style="color:red">**We are still updating the content**</span>

Our research themes spam in the following areas. We are interested in developing novel algorithms and efficient computational solutions for challenging research questions.
- [Robust and Adversarial Machine Learning](#Robust)
- [Optimization of Tuning and Training ML Models](#Tune)
- [Deep Model Inferences on Edge Devices](#EdgeInf)
- [Synthetic Data Generation](#GAN)
- [Federated Learning Systems](#Federated)
- [Tucker Tensor Decomposition](#Tucker)
- [Fair Information Maximization on Social Media](#FairIM)


## Robust and Adversarial Machine Learning<a name="Robust"></a> 

**Research questions**: Can today's deep neural networks handle noisy data sets, namely corrupted inputs and labels? How to design novel learning algorithms to dstill the data quality and enhance the robustness of learning models when encountering noisy and adversarial input? 

{: .box-note}
We are working on noise resilient learning frameworks, leveraging adversarial examples, expert judgement, and robust loss functions.

<a href="#top">
         <img alt="top" src="./images/top.png" width="40" height="40">
</a>



## Optimization for Machine Learning Services and Clusters<a name="Tune"></a> 
-----
**Research questions**: Training deep models consumes tremendous computing time and resources; however tuning the hyperparameter of deep models is even multiple fold higher. Can one design efficient and accurate tuning framework for deep neural networks such that the optimal parameters can be found at minimum computational resources?

{: .box-note}
We are working on accelerating processing strategies that only execute critical data and leverage the workload similarities when tuning hyperparameters and training a wide range of ML models.  


<a href="#top">
         <img alt="top" src="/images/top.png" width="40" height="40">
</a>


## Deep Model Inferences on Edge Devices<a name="EdgeInf"></a> 
----
**Research questions**: How to choose suitable trained models from the plethoral of existing ones and deploy at the edge devices? How to optimize the performance of deep models on the edge devices? Can today's edge devices efficiently execute multiple DNNs at the same time, e,.g., extracting information of people, aged and gender from images?

{: .box-note}
We are working on various scheduling and model selection algorithms to adaptively run multiple DNNs on resource limited edge devices, in fulfilling various usersâ€™ requirements. 
 

<a href="#top">
         <img alt="top" src="/images/top.png" width="40" height="40">
</a>


## Synthetic Data Generator<a name="GAN"></a> 
-----
**Research questions**: While big data is powering up the deep learning models, it is costly and inevitably intrudes privacy to curate such data. Synthetically generated data not only alleviates the cost of collecting data but also overcome the privacy concerns and legislation boundary.

{: .box-note}
We are working on various synthetic table generation methods for table and image data. We are in collaboration with companies in the finance sector. 

<a href="#top">
         <img alt="top" src="/images/top.png" width="40" height="40">
</a>


## Federated Learning Systems: Incentive and Backdoor<a name="Federated"></a> 
-----
**Research questions**: Federated learning framework preserves privacy by design as user data stays on devices. How to provide the incentives for users at the federated learning systems? How to value the contributed models from other users? Can we trust the models provided by other users?

{: .box-note}
We are designing incentive mechanisms and defense strategies against backdoor attacks. 

<a href="#top">
         <img alt="top" src="/images/top.png" width="40" height="40">
</a>


## Tucker Tensor Decomposition (STD)<a name="Tucker"></a> 
-----
**Research questions**: Sparse Tucker decomposition is widely used in low-rank representation learning for sparse big data analysis. Due to the entanglement problem  of  core  tensor  and  factor  matrices,  the  computational process  for  STD  faces  the challenge of   intermediate  variables  explosion. How to design an efficient optimization algorithm for STD without degrading approximation accuracy?

{: .box-note}
We are working on various optimization techniques to accelerate the sparse matrix factorization, particularly for tucker tensor decomposition.

<a href="#top">
         <img alt="top" src="./images/top.png" width="40" height="40">
</a>


## Fair Information Maximization on Social Media<a name="FairIM"></a> 
-----
**Research questions**: Users on social media with high visibility are often selected as seeds to spread information and affect their adoption in target groups. Even though female users are more active on social media than male users, males are regarded as influential in various centrality measures.

{: .box-note}
We are trying to answer how gender differences and similarities can impact the information spreading process on social media. We are developing a disparity-aware seeding algorithms.

<a href="#top">
         <img alt="top" src="/images/top.png" width="40" height="40">
</a>



